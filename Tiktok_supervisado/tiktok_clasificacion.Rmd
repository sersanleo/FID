---
title: "Tiktok_clasificacion"
author: "Sergio Sánchez León"
date: "2/1/2022"
output: html_document
---

Primero leo los datos.
```{r}
tiktok = read.csv('../Tiktok_pre/tiktok_processed.csv')
```

Importo las librerías utilizadas:
```{r,warning=FALSE,message=FALSE}
library(rpart)
library(rattle)
library(RColorBrewer)
library(arules)
library(ROCR)
library(caret)
library(class)
library(gbm)
library(glmnet)
library(xgboost)
```

Añadimos una nueva columna que clasifique los datos en muy bailables y poco bailables.
```{r}
tiktok$danceability_class = discretize(tiktok$danceability, method="interval", breaks=2, labels=FALSE)
summary(tiktok$danceability_class)
```

Normalizamos duration.
```{r}
tiktok$duration <- predict(preProcess(as.data.frame(tiktok$duration), method =
                                        c("range")),
                           as.data.frame(tiktok$duration))[, "tiktok$duration"]
```

Se eliminan las columnas que no se van a utilizar (no deben ser utilizadas para clasificar la categoría de la danzabilidad de una canción):
```{r}
tiktok$track_name = NULL
tiktok$artist_id = NULL
tiktok$release_date = NULL
tiktok$danceability = NULL
tiktok$key = NULL
tiktok$genre__TIKTOK = NULL
tiktok$genre_TIKTOK.DANCE = NULL
tiktok$genre_TIKTOK.OPM = NULL
tiktok$genre_TIKTOK.PHILIPPINES = NULL
```

# ARBOLES

Dividimos el conjuntos de datos en dos: conjunto de entrenamiento (70%) y conjunto de prueba o "test" (30%), y se construye el árbol de decisión con la función rpart utilizando el conjunto de datos de entrenamiento:
```{r}
set.seed(1)
# Esta función de caret permite crear particiones con una proporción equilibrada de los distintos valores de cada clase
intrain<-createDataPartition(y=tiktok$danceability_class,p=0.7,list=FALSE)

train<-tiktok[intrain,]
test<-tiktok[-intrain,]

set.seed(1)
tree <- rpart(danceability_class~., train, method = "class")
# Dibuja el árbol
fancyRpartPlot(tree, cex=0.53)
```

```{r}
# Predice los valores del conjunto de test
pred <- predict(tree, test, type = "class")
```

```{r}
# Construye la matriz de confusion
conf <- table(test$danceability_class, pred)
conf
# Calcula accuracy
acc <- sum(diag(conf)) / sum(conf)
acc
# Calcula sensibility
sen <- conf[2,2] / sum(conf[2,])
sen
# Calcula specificity
spec <- conf[1,1] / sum(conf[1,])
spec
```
Se construye el árbol con la heurística de la ganancia de información:
```{r}
set.seed(1)
# Cambia la llamada a rpart para utilizar como heurística la ganancia de información
tree_i <- rpart(danceability_class~., train, method = "class", parms = list(split = "information"))
# Dibuja árbol
fancyRpartPlot(tree_i, cex=0.45)
```

Se construye nuevamente la matriz de confusión y se calculan las medidas del nuevo árbol: 
```{r}
# Predice los valores del conjunto de test
pred_i <- predict(tree_i, test, type = "class")
# Construye la matriz de confusion
conf_i <- table(test$danceability_class, pred_i)
conf_i
# Calcula accuracy
acc_i <- sum(diag(conf_i)) / sum(conf_i)
acc_i
# Calcula sensibility
sen_i <- conf_i[2,2] / sum(conf_i[2,])
sen_i
# Calcula specificity
spec_i <- conf_i[1,1] / sum(conf_i[1,])
spec_i
```

Ahora podamos el árbol de mejores resultados (el último) para mejorar su interpretabilidad, y recalculamos:
```{r}
# Podamos el árbol
pruned <- prune(tree, cp = 0.015)
fancyRpartPlot(pruned)
# Predice los valores del conjunto de test
pred_pruned <- predict(pruned, test, type = "class")
# Construye la matriz de confusion
conf_pruned <- table(test$danceability_class, pred_pruned)
conf_pruned
# Calcula accuracy
acc_pruned <- sum(diag(conf_pruned)) / sum(conf_pruned)
acc_pruned
# Calcula sensibility
sen_pruned <- conf_pruned[2,2] / sum(conf_pruned[2,])
sen_pruned
# Calcula specificity
spec_pruned <- conf_pruned[1,1] / sum(conf_pruned[1,])
spec_pruned
```

## Análisis ROC

Ahora vamos a predecir valores de probabilidad utilizando el árbol anterior y realizar análisis ROC.
```{r}
all_probs <- predict(pruned, test, type="prob")
summary(all_probs)

# Make a prediction object: predictions, usando "muy bailable" como si fuera "es bailable"
predictions <- prediction(all_probs[,2], test$danceability_class)

# Make a performance object: perf
performances <- performance(predictions, "tpr", "fpr")

# Plot this curve
plot(performances)

# AUC
auc <- performance(predictions, "auc")
print(auc@y.values[[1]])
```
AUC está dentro de [0.6, 0.75): Test regular.

Ahora vamos a probar con el árbol sin podar:
```{r}
all_probs <- predict(tree_i, test, type="prob")
summary(all_probs)

predictions <- prediction(all_probs[,2], test$danceability_class)
performances <- performance(predictions, "tpr", "fpr")

plot(performances)

auc <- performance(predictions, "auc")
print(auc@y.values[[1]])
```

El resultado sigue estando dentro del intervalo anterior (es regular), pero ha mejorado considerablemente.

Por último, se prueba con el árbol que no ha sido construido usando el método de la ganancia de información:
```{r}
all_probs <- predict(tree, test, type="prob")
summary(all_probs)

predictions <- prediction(all_probs[,2], test$danceability_class)
performances <- performance(predictions, "tpr", "fpr")

plot(performances)

auc <- performance(predictions, "auc")
print(auc@y.values[[1]])
```

Vemos que este consigue mejorar sensiblemente el AUC.


# kNN

Ahora vamos a utilizar la técnica de clasificación kNN.
```{r}
knn_train = train
knn_test = test

train_labels = knn_train$danceability_class
test_labels = knn_test$danceability_class
  
knn_train$danceability_class = NULL
knn_test$danceability_class = NULL
```


```{r}
set.seed(1)
knn_pred <- knn(train = knn_train, test = knn_test, cl = train_labels, k=5)

knn_tab <- table(test_labels, knn_pred)
knn_tab

# Calcula accuracy
knn_accuracy <- sum(diag(knn_tab)) / sum(knn_tab)
knn_accuracy
# Calcula sensibility
knn_sensibility <- knn_tab[2,2] / sum(knn_tab[2,])
knn_sensibility
# Calcula specificity
knn_specificity <- knn_tab[1,1] / sum(knn_tab[1,])
knn_specificity
```

Calculamos la mejor k:
```{r}
set.seed(1)

range <- 1:round(0.02 * nrow(knn_train))
accs <- rep(0, length(range))
sens <- rep(0, length(range))
specs <- rep(0, length(range))

for (k in range) {
  
  pred <- knn(knn_train, knn_test, train_labels, k = k)
  
  conf <- table(test_labels, pred)
  
  accs[k] <- sum(diag(conf)) / sum(conf)
  sens[k] <- conf[2,2] / sum(conf[2,])
  specs[k] <- conf[1,1] / sum(conf[1,])
}

# Plot the accuracies. Title of x-axis is "k".
plot(range, accs, xlab = "k", ylab="accuracy")
plot(range, sens, xlab = "k", ylab="sensibility")
plot(range, specs, xlab = "k", ylab="specificity")

# Calculate the best k
print('Mejor k en función de la accuracy')
which.max(accs)
accs[which.max(accs)]
sens[which.max(accs)]
specs[which.max(accs)]
print('Mejor k en función de la sensibilidad')
which.max(sens)
accs[which.max(sens)]
sens[which.max(sens)]
specs[which.max(sens)]
print('Mejor k en función de la especificidad')
which.max(specs)
accs[which.max(specs)]
sens[which.max(specs)]
specs[which.max(specs)]
```




# CARET

```{r}
tiktok_caret = merge(train, test)

tiktok_caret$danceability_class = as.factor(sapply(tiktok_caret$danceability_class, function(x) return(if (x == 1) "Imbailable" else "Bailable")))
```

glm
```{r}
set.seed(1)

fitControl = trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  savePredictions = TRUE,
  verbose = FALSE
)
glmFit <- train(danceability_class~., tiktok_caret, method = "glm", trControl = fitControl)
glmFit
```

glmnet
```{r}
set.seed(1)

glmNetFit <-
  train(
    danceability_class ~ .,
    tiktok_caret,
    method = "glmnet",
    trControl = fitControl
  )
glmNetFit
```

glmNetTuning
```{r}
set.seed(1)

glmNetTuningFit <-
  train(
    danceability_class ~ .,
    tiktok_caret,
    method = "glmnet",
    tuneGrid = expand.grid(alpha = 0:1, lambda = seq(0.0001, 1, length = 20)),
    trControl = fitControl
  )
glmNetTuningFit
```


xgbTree
```{r,results=FALSE,warning=FALSE,message=FALSE}
set.seed(1)

xgbTreeFit <-
  train(
    danceability_class ~ .,
    tiktok_caret,
    method = "xgbTree",
    trControl = fitControl
  )
```
```{r}
xgbTreeFit
```


## Comparativa de modelos

Para comparar modelos Caret hace automáticamente un remuestreo para comparar distintos modelos en igualdad de condiciones. Esto lo hace con la función resamples a la que le pasamos una lista con los modelos generados anteriormente.

```{r}
model_list <- list(
  glm = glmFit,
  glmnet = glmNetFit,
  glmnet_tunning = glmNetTuningFit,
  xgbTree = xgbTreeFit
)

# Pass model_list to resamples(): resamples
resamples <- resamples(model_list)
```


Finalmente, vamos a mostrar gráficamente los resultados obtenidos, con lo que de manera visual podremos decidir cuál es el mejor modelo.
```{r}
# Summarize the results
summary(resamples)

# GENIAL PARA VER COMPARATIVA DE MANERA VISUAL
bwplot(resamples, metric = "ROC") # display univariate visualizations of the resampling distributions
dotplot(resamples, metric="ROC")
```